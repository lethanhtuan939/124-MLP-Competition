{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns before processing: (255080, 22)\n",
      "X_test columns before processing: (63771, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramFiles\\Anaconda\\envs\\ai_class\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "d:\\ProgramFiles\\Anaconda\\envs\\ai_class\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after scaling: (255080, 22)\n",
      "X_test shape after scaling: (63771, 22)\n",
      "RandomForest RMSE: 53.67793232245581\n",
      "Epoch 1/50\n",
      "6377/6377 [==============================] - 27s 4ms/step - loss: 10790.4453 - val_loss: 4634.7256 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "6377/6377 [==============================] - 25s 4ms/step - loss: 9427.4609 - val_loss: 3882.1521 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "6377/6377 [==============================] - 23s 4ms/step - loss: 8850.6738 - val_loss: 7875.4531 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "6377/6377 [==============================] - 28s 4ms/step - loss: 8395.9678 - val_loss: 5594.6475 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "6377/6377 [==============================] - 29s 5ms/step - loss: 8706.9766 - val_loss: 6754.3491 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "6377/6377 [==============================] - 28s 4ms/step - loss: 7840.3540 - val_loss: 7864.2954 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "6377/6377 [==============================] - 27s 4ms/step - loss: 6993.3730 - val_loss: 3524.6084 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "6377/6377 [==============================] - 28s 4ms/step - loss: 6898.4863 - val_loss: 5340.7427 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "6377/6377 [==============================] - 21s 3ms/step - loss: 6692.5000 - val_loss: 9849.6318 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "6377/6377 [==============================] - 24s 4ms/step - loss: 6418.6382 - val_loss: 4624.3369 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "6377/6377 [==============================] - 21s 3ms/step - loss: 6398.9551 - val_loss: 4790.6733 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "6377/6377 [==============================] - 36s 6ms/step - loss: 5980.4473 - val_loss: 5918.5327 - lr: 2.5000e-04\n",
      "Epoch 13/50\n",
      "6377/6377 [==============================] - 28s 4ms/step - loss: 5925.6235 - val_loss: 3870.8000 - lr: 2.5000e-04\n",
      "Epoch 14/50\n",
      "6377/6377 [==============================] - 35s 6ms/step - loss: 5720.6919 - val_loss: 3651.5486 - lr: 2.5000e-04\n",
      "Epoch 15/50\n",
      "6377/6377 [==============================] - 29s 5ms/step - loss: 5696.5425 - val_loss: 5076.0938 - lr: 2.5000e-04\n",
      "MLP with RF Feature RMSE: 59.368402394551794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define an advanced RMSE function for logging\n",
    "def print_rmse(y_true, y_pred, model_name=\"Model\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"{model_name} RMSE: {rmse}\")\n",
    "    return rmse\n",
    "\n",
    "# Load, process data, and add advanced features\n",
    "def load_and_process_data(train_path, target_path, test_path, test_size=0.2, random_state=42):\n",
    "    # Load the data\n",
    "    X_train = pd.read_csv(train_path, low_memory=False).drop(columns=['ID'], errors='ignore')\n",
    "    y_train = pd.read_csv(target_path)['TARGET']\n",
    "    X_test = pd.read_csv(test_path, low_memory=False)\n",
    "    \n",
    "    # Drop the 'ID' column from test data if it exists\n",
    "    test_ids = X_test.pop('ID') if 'ID' in X_test.columns else np.arange(len(X_test))\n",
    "\n",
    "    # Process both train and test datasets\n",
    "    for df in [X_train, X_test]:\n",
    "        # Convert 'tradeTime' to datetime\n",
    "        if 'tradeTime' in df.columns:\n",
    "            df['tradeTime'] = pd.to_datetime(df['tradeTime'], errors='coerce')\n",
    "        \n",
    "        # Replace categorical values as needed\n",
    "        df.replace({\n",
    "            'elevator': {1: 'has elevator', 0: 'no elevator'},\n",
    "            'subway': {1: 'has subway', 0: 'no subway'},\n",
    "            'buildingStructure': {1: 'unknown', 2: 'mixed', 3: 'brick and wood', 4: 'concrete', 5: 'steel', 6: 'steel-concrete composite'},\n",
    "            'renovationCondition': {1: 'other', 2: 'rough', 3: 'Simplicity', 4: 'hardcover'},\n",
    "            'buildingType': {1: 'tower', 2: 'bungalow', 3: 'combination of plate and tower', 4: 'plate'}\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Ensure 'constructionTime' is properly numeric and clean up invalid values\n",
    "        if 'constructionTime' in df.columns:\n",
    "            df['constructionTime'] = pd.to_numeric(df['constructionTime'], errors='coerce')  # Force invalid to NaN\n",
    "\n",
    "        # Convert floor information to numeric\n",
    "        if 'floor' in df.columns:\n",
    "            df['floor'] = df['floor'].astype(str).str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype(int)\n",
    "\n",
    "        # Convert Lat and Lng to numeric and calculate distance to the capital\n",
    "        df['Lat'] = pd.to_numeric(df['Lat'], errors='coerce')\n",
    "        df['Lng'] = pd.to_numeric(df['Lng'], errors='coerce')\n",
    "\n",
    "        capital_Lng = np.radians(116.4074)\n",
    "        capital_Lat = np.radians(39.9042)\n",
    "        df['distanceToCapital'] = np.arccos(\n",
    "            np.sin(np.radians(df['Lat'])) * np.sin(capital_Lat) +\n",
    "            np.cos(np.radians(df['Lat'])) * np.cos(capital_Lat) *\n",
    "            np.cos(capital_Lng - np.radians(df['Lng']))\n",
    "        ) * 6371.0088  # Earth's radius in kilometers\n",
    "\n",
    "        # Add age of the building\n",
    "        if 'tradeTime' in df.columns and 'constructionTime' in df.columns:\n",
    "            df['ageOfBuilding'] = df['tradeTime'].dt.year - df['constructionTime']\n",
    "\n",
    "    # Handle missing values for 'elevator' and 'subway'\n",
    "    for df in [X_train, X_test]:\n",
    "        df['elevator'] = df['elevator'].fillna(0)  # Assuming no elevator if missing\n",
    "        df['subway'] = df['subway'].fillna(0)  # Assuming no subway access if missing\n",
    "\n",
    "    # Ensure all columns are numeric where expected and handle NaNs\n",
    "    X_train = X_train.apply(pd.to_numeric, errors='coerce')  # Force conversion of all columns to numeric\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    print(f\"X_train columns before processing: {X_train.shape}\")\n",
    "    print(f\"X_test columns before processing: {X_test.shape}\")\n",
    "    \n",
    "    # Align columns between X_train and X_test to avoid mismatches\n",
    "    common_cols = X_train.columns.intersection(X_test.columns)\n",
    "    X_train, X_test = X_train[common_cols], X_test[common_cols]\n",
    "    \n",
    "    # Add imputation and scaling\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = PowerTransformer()\n",
    "\n",
    "    # Impute and scale both training and test sets\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(imputer.fit_transform(X_train)), columns=common_cols)\n",
    "    X_test = pd.DataFrame(scaler.transform(imputer.transform(X_test)), columns=common_cols)\n",
    "    \n",
    "    print(f\"X_train shape after scaling: {X_train.shape}\")\n",
    "    print(f\"X_test shape after scaling: {X_test.shape}\")\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train_split, X_val_split, y_train_split, y_val_split, X_test, test_ids\n",
    "\n",
    "def build_mlp_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train MLP model\n",
    "def compile_and_train_mlp(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train a RandomForest model\n",
    "def train_random_forest(X_train, y_train, X_val, y_val):\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=500, \n",
    "        max_depth=15, \n",
    "        min_samples_split=5, \n",
    "        min_samples_leaf=2, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred_val = rf_model.predict(X_val)\n",
    "\n",
    "    # Print and return RMSE\n",
    "    val_rmse = print_rmse(y_val, y_pred_val, model_name=\"RandomForest\")\n",
    "    \n",
    "    return rf_model, val_rmse\n",
    "\n",
    "# Save predictions for submission\n",
    "def save_submission(y_pred, test_ids, filename='Latest_submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        \"ID\": test_ids,\n",
    "        \"TARGET\": y_pred\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "\n",
    "# Load, process, and train\n",
    "X_train_split, X_val_split, y_train_split, y_val_split, X_test_scaled, test_ids = load_and_process_data(\n",
    "    'data/X_train.csv', 'data/y_train.csv', 'data/X_test.csv'\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model, rf_rmse = train_random_forest(X_train_split, y_train_split, X_val_split, y_val_split)\n",
    "\n",
    "# Use Random Forest predictions as a new feature for MLP\n",
    "rf_train_predictions = rf_model.predict(X_train_split).reshape(-1, 1)\n",
    "rf_val_predictions = rf_model.predict(X_val_split).reshape(-1, 1)\n",
    "rf_test_predictions = rf_model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "# Append RF predictions to training and validation sets\n",
    "X_train_split_with_rf = np.hstack((X_train_split, rf_train_predictions))\n",
    "X_val_split_with_rf = np.hstack((X_val_split, rf_val_predictions))\n",
    "X_test_with_rf = np.hstack((X_test_scaled, rf_test_predictions))\n",
    "\n",
    "# Train MLP model using RandomForest predictions as an additional feature\n",
    "mlp_model = build_mlp_model(X_train_split_with_rf.shape[1])\n",
    "mlp_model = compile_and_train_mlp(mlp_model, X_train_split_with_rf, y_train_split, X_val_split_with_rf, y_val_split)\n",
    "\n",
    "# Predict using the trained MLP model on the validation set\n",
    "y_pred_val_mlp = mlp_model.predict(X_val_split_with_rf).flatten()\n",
    "mlp_rmse = print_rmse(y_val_split, y_pred_val_mlp, model_name=\"MLP with RF Feature\")\n",
    "\n",
    "# Predict on the test set using the trained MLP model\n",
    "y_pred_test_mlp = mlp_model.predict(X_test_with_rf).flatten()\n",
    "\n",
    "# Save predictions (final submission)\n",
    "save_submission(y_pred_test_mlp, test_ids, filename='Latest_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
